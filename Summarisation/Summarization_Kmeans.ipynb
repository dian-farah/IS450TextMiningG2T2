{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ivyha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ivyha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import spatial\n",
    "import networkx as nx\n",
    "\n",
    "import nltk\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "from rouge_score import rouge_scorer\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('billsum_v4_1/ca_test_data_final_OFFICIAL.jsonl', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_semicolon(text, threshold=10):\n",
    "    '''\n",
    "    Get rid of semicolons.\n",
    "    First split text into fragments between the semicolons. If the fragment \n",
    "    is longer than the threshold, turn the semicolon into a period. O.w treat\n",
    "    it as a comma.\n",
    "    Returns new text\n",
    "    '''\n",
    "    new_text = \"\"\n",
    "    for subset in re.split(';', text):\n",
    "        subset = subset.strip() # Clear off spaces\n",
    "        # Check word count\n",
    "        if len(subset.split()) > threshold:\n",
    "            # Turn first char into uppercase\n",
    "            new_text += \". \" + subset[0].upper() + subset[1:]\n",
    "        else:\n",
    "            # Just append with a comma \n",
    "            new_text += \", \" + subset\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "USC_re = re.compile('[Uu]\\.*[Ss]\\.*[Cc]\\.]+')\n",
    "PAREN_re = re.compile('\\([^(]+\\ [^\\(]+\\)')\n",
    "BAD_PUNCT_RE = re.compile(r'([%s])' % re.escape('\"#%&\\*\\+/<=>@[\\]^{|}~_'), re.UNICODE)\n",
    "BULLET_RE = re.compile('\\n[\\ \\t]*`*\\([a-zA-Z0-9]*\\)')\n",
    "DASH_RE = re.compile('--+')\n",
    "WHITESPACE_RE = re.compile('\\s+')\n",
    "EMPTY_SENT_RE = re.compile('[,\\.]\\ *[\\.,]')\n",
    "FIX_START_RE = re.compile('^[^A-Za-z]*')\n",
    "FIX_PERIOD = re.compile('\\.([A-Za-z])')\n",
    "SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\\.|\\nSEC\\.* [0-9]{1,2}\\.|Sec\\.* [0-9]{1,2}\\.')\n",
    "\n",
    "FIX_PERIOD = re.compile('\\.([A-Za-z])')\n",
    "\n",
    "SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\\.|\\nSEC\\.* [0-9]{1,2}\\.|Sec\\.* [0-9]{1,2}\\.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Borrowed from the FNDS text processing with additional logic added in.\n",
    "    Note: we do not take care of token breaking - assume SPACY's tokenizer\n",
    "    will handle this for us.\n",
    "    \"\"\"\n",
    "\n",
    "    # Indicate section headers, we need them for features\n",
    "    text = SECTION_HEADER_RE.sub('SECTION-HEADER', text)\n",
    "    # For simplicity later, remove '.' from most common acronym\n",
    "    text = text.replace(\"U.S.\", \"US\")\n",
    "    text = text.replace('SEC.', 'Section')\n",
    "    text = text.replace('Sec.', 'Section')\n",
    "    text = USC_re.sub('USC', text)\n",
    "\n",
    "    # Remove parantheticals because they are almost always references to laws \n",
    "    # We could add a special tag, but we just remove for now\n",
    "    # Note we dont get rid of nested parens because that is a complex re\n",
    "    #text = PAREN_re.sub('LAWREF', text)\n",
    "    text = PAREN_re.sub('', text)\n",
    "    \n",
    "\n",
    "    # Get rid of enums as bullets or ` as bullets\n",
    "    text = BULLET_RE.sub(' ',text)\n",
    "    \n",
    "    # Clean html \n",
    "    text = text.replace('&lt;all&gt;', '')\n",
    "\n",
    "    # Remove annoying punctuation, that's not relevant\n",
    "    text = BAD_PUNCT_RE.sub('', text)\n",
    "\n",
    "    # Get rid of long sequences of dashes - these are formating\n",
    "    text = DASH_RE.sub( ' ', text)\n",
    "\n",
    "    # removing newlines, tabs, and extra spaces.\n",
    "    text = WHITESPACE_RE.sub(' ', text)\n",
    "    \n",
    "    # If we ended up with \"empty\" sentences - get rid of them.\n",
    "    text = EMPTY_SENT_RE.sub('.', text)\n",
    "    \n",
    "    # Attempt to create sentences from bullets \n",
    "    text = replace_semicolon(text)\n",
    "    \n",
    "    # Fix weird period issues + start of text weirdness\n",
    "    #text = re.sub('\\.(?=[A-Z])', '  . ', text)\n",
    "    # Get rid of anything thats not a word from the start of the text\n",
    "    text = FIX_START_RE.sub( '', text)\n",
    "    # Sometimes periods get formatted weird, make sure there is a space between periods and start of sent   \n",
    "    text = FIX_PERIOD.sub(\". \\g<1>\", text)\n",
    "\n",
    "    # Fix quotes\n",
    "    text = text.replace('``', '\"')\n",
    "    text = text.replace('\\'\\'', '\"')\n",
    "\n",
    "    # Add special punct back in\n",
    "    text = text.replace('SECTION-HEADER', '')\n",
    "\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data.text.map(clean_text)\n",
    "        \n",
    "data['clean_summary'] = data.summary.map(clean_text)\n",
    "\n",
    "data['clean_title'] = data.title.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The people State California enact follows: <SECTION-HEADER> The Legislature finds declares following: (1) Since 1899 congressionally chartered veterans’ organizations provided valuable service nation’s returning service members. These organizations help preserve memories incidents great hostilities fought nation, preserve strengthen comradeship members. These veterans’ organizations manage properties including lodges, posts, fraternal halls. These properties act safe haven veterans ages families gather camaraderie fellowship, share stories, seek support people understand unique experiences. This aids healing process returning veterans, ensures health happiness. As result congressional chartering veterans’ organizations, United States Internal Revenue Service created special tax exemption organizations Section 501(c)(19) Internal Revenue Code. Section 501(c)(19) Internal Revenue Code related federal regulations provide exemption posts organizations war veterans, auxiliary unit society of, trust foundation for, post organization that, attributes, carries programs perpetuate memory deceased veterans members Armed Forces comfort survivors, conducts programs religious, charitable, scientific, literary, educational purposes, sponsors participates activities patriotic nature, provides social recreational activities members. Section 215.1 Revenue Taxation Code stipulates buildings, support real property buildings situated required convenient use occupation buildings, exclusively charitable purposes, owned veterans’ organization chartered Congress United States, organized operated charitable purposes, solely exclusively purpose organization, conducted profit net earnings ensures benefit private individual member thereof, exempt taxation. The Chief Counsel State Board Equalization concluded, based 1979 appellate court decision, parts American Legion halls exempt property taxation parts, billiard rooms, card rooms, similar areas, exempt. In 1994 memorandum, State Board Equalization’s legal division concluded areas normally considered eligible exemptions office areas counsel veterans area store veterans’ records, meeting hall bar facilities considered charitable purposes. Tax-exempt status intended provide economic incentive support veterans’ organizations provide social welfare community current military personnel. The State Board Equalization’s constriction tax exemption resulted onerous tax burden California veteran service organizations posts halls, hinders posts’ ability provide facilities veterans, threatens economic viability local organizations. The charitable activities veteran service organizations post hall counseling veterans. The requirements listed qualification federal tax exemption clearly dictate need office. Programs perpetuate memory deceased veterans members Armed Forces comfort survivors require use facilities funerals receptions. Programs religious, charitable, scientific, literary, educational purposes require space 50 attendees. Activities patriotic nature need facilities accommodate hundreds people. Social recreational activities members require precisely areas considered “not charitable purposes” State Board Equalization. The State Board Equalization’s interpretation Revenue Taxation Code reflects lack understanding purpose programs veterans service organizations posts halls detrimental good works performed support veteran community. <SECTION-HEADER><SECTION-HEADER> Section 215.1 Revenue Taxation Code amended read: 215.1. All buildings, real property buildings situated required convenient use occupation buildings, exclusively charitable purposes, owned veterans’ organization chartered Congress United States, organized operated charitable purposes, exempt federal income tax organization described Section 501(c)(19) Internal Revenue Code solely exclusively purpose organization, conducted profit net earnings inures benefit private individual member thereof, shall exempt taxation. The exemption provided section shall apply property organizations meeting requirements section, subdivision (b) Section 4 Article XIII California Constitution, paragraphs (1) (4), inclusive, (6), (7) subdivision (a) Section 214. (1) The exemption specified subdivision (a) shall denied property basis property fraternal, lodge, social club purposes. With regard subdivision, Legislature finds declares following: The exempt activities veterans’ organization described subdivision (a) qualitatively differ exempt activities nonprofit entities use property fraternal, lodge, social club purposes exempt purpose veterans’ organization conduct programs perpetuate memory deceased veterans members Armed Forces comfort survivors, conduct programs religious, charitable, scientific, literary, educational purposes, sponsor participate activities patriotic nature, provide social recreational activities members. In light distinction, use real property veterans’ organization described subdivision (a), fraternal, lodge, social club purposes central organization’s exempt purposes activities. In light factors set forth subparagraphs (A) (B), use real property veterans’ organization described subdivision (a) fraternal, lodge, social club purposes, constitutes exclusive use property charitable purpose meaning subdivision (b) Section 4 Article XIII California Constitution. The exemption provided section shall apply portion property consists bar alcoholic beverages served. The portion property ineligible veterans’ organization exemption shall area primarily prepare serve alcoholic beverages. An organization files claim exemption provided section shall file assessor valid organizational clearance certificate issued pursuant Section 254.6. This exemption shall known “veterans’ organization exemption.”<SECTION-HEADER><SECTION-HEADER> Notwithstanding Section 2229 Revenue Taxation Code, appropriation act state shall reimburse local agency property tax revenues lost pursuant act. <SECTION-HEADER><SECTION-HEADER> This act provides tax levy meaning Article IV Constitution shall immediate effect.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means\n",
    "\n",
    "References:\n",
    "https://medium.com/@akankshagupta371/understanding-text-summarization-using-k-means-clustering-6487d5d37255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Sentences\n",
    "sentences = []\n",
    "for doc in data['clean_text']:\n",
    "  sentences.append(sent_tokenize(doc))\n",
    "\n",
    "#k_sentences = [y for x in k_sentences for y in x] # flatten list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_sentences = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in k_sentences:\n",
    "    for s in doc:\n",
    "        re.sub('[^a-zA-Z]`', \" \", s)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_sentences = [[s.lower() for s in doc] for doc in k_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_sentences = [[[words for words in sentence.split(' ') if words not in stop_words] for sentence in doc] for doc in k_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_sentences = [list(map(\" \".join,doc))for doc in k_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_all_words = [[s.split() for s in doc] for doc in k_sentences]\n",
    "k_model = [Word2Vec(doc, min_count = 1, vector_size = 300,epochs=100)  for doc in k_all_words]\n",
    "\n",
    "#[[Word2Vec(sent, vector_size=1, min_count=1) for sent in doc] for doc in pagerank_all_senttoken]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_sent_vector = []\n",
    "\n",
    "doc_vector = []\n",
    "\n",
    "for i in range(len(k_sentences)):\n",
    "    for s in k_sentences[i]:\n",
    "        plus = 0\n",
    "        for j in s.split():\n",
    "            plus += k_model[i].wv[j]\n",
    "        plus = plus/len(s.split())\n",
    "\n",
    "        doc_vector.append(plus)\n",
    "    k_sent_vector.append(doc_vector)\n",
    "    doc_vector = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters, init = 'k-means++', random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(doc)\n",
    "    Sum_of_squared_distances.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_kmeans = [kmeans.fit_predict(doc) for doc in k_sent_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list = []\n",
    "my_list=[]\n",
    "for h in range(len(y_kmeans)):\n",
    "    for i in range(n_clusters):\n",
    "        my_dict={}\n",
    "        for j in range(len(y_kmeans[h])):\n",
    "            \n",
    "            if y_kmeans[h][j]==i:\n",
    "                my_dict[j] =  distance.euclidean(kmeans.cluster_centers_[i],k_sent_vector[h][j])\n",
    "        min_distance = min(my_dict.values())\n",
    "        sent_list.append(min(my_dict, key=my_dict.get))\n",
    "    my_list.append(sent_list)\n",
    "    sent_list = []\n",
    " \n",
    "for doc in my_list:\n",
    "    for s in sorted(doc):\n",
    "        print(sentences[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_train_sum = [[sentences[i][j] for j in sorted(my_list[i])] for i in range(len(my_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_train_sum = list(map(\" \".join, k_train_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['k_sum'] = k_train_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Evaluation scores\n",
    "\n",
    "References: https://towardsdatascience.com/the-ultimate-performance-metric-in-nlp-111df6c64460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = data['clean_summary'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.1411046294012602,\n",
       "  'p': 0.31130387689391087,\n",
       "  'f': 0.18326353871071346},\n",
       " 'rouge-2': {'r': 0.026603814750615972,\n",
       "  'p': 0.08038325278963127,\n",
       "  'f': 0.03708949273132507},\n",
       " 'rouge-l': {'r': 0.13305651294005308,\n",
       "  'p': 0.29448271643561413,\n",
       "  'f': 0.17296877639168165}}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "\n",
    "Scores = rouge.get_scores(k_train_sum, summary, avg=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "097a94cc1bfc5a0a380b63d1eb45dc603bb0f63840061a2276c4020d01d7c102"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
