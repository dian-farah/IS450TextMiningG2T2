{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dianfarahr.2019\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import tokenize\n",
    "from operator import itemgetter\n",
    "import math\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.edmundson import EdmundsonSummarizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import spatial\n",
    "from scipy.sparse.linalg import svds\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import rouge\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contract</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "      <td>section 1. short title. this act may be cited ...</td>\n",
       "      <td>national science education tax incentive for b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "      <td>section 1. short title. this act may be cited ...</td>\n",
       "      <td>small business expansion and hiring act of 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A bill to require the Director of National Int...</td>\n",
       "      <td>section 1. release of documents captured in ir...</td>\n",
       "      <td>requires the director of national intelligence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A bill to improve data collection and dissemin...</td>\n",
       "      <td>section 1. short title. this act may be cited ...</td>\n",
       "      <td>national cancer act of 2003 - amends the publi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A bill to amend the Internal Revenue Code of 1...</td>\n",
       "      <td>section 1. short title. this act may be cited ...</td>\n",
       "      <td>military call-up relief act - amends the inter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            contract  \\\n",
       "0  To amend the Internal Revenue Code of 1986 to ...   \n",
       "1  To amend the Internal Revenue Code of 1986 to ...   \n",
       "2  A bill to require the Director of National Int...   \n",
       "3  A bill to improve data collection and dissemin...   \n",
       "4  A bill to amend the Internal Revenue Code of 1...   \n",
       "\n",
       "                                                text  \\\n",
       "0  section 1. short title. this act may be cited ...   \n",
       "1  section 1. short title. this act may be cited ...   \n",
       "2  section 1. release of documents captured in ir...   \n",
       "3  section 1. short title. this act may be cited ...   \n",
       "4  section 1. short title. this act may be cited ...   \n",
       "\n",
       "                                             summary  \n",
       "0  national science education tax incentive for b...  \n",
       "1  small business expansion and hiring act of 201...  \n",
       "2  requires the director of national intelligence...  \n",
       "3  national cancer act of 2003 - amends the publi...  \n",
       "4  military call-up relief act - amends the inter...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billsum_test = pd.read_excel('../data/billsum_test.xlsx')\n",
    "\n",
    "billsum_test['text'] = billsum_test['text'].apply(lambda x: x.lower())\n",
    "billsum_test['summary'] = billsum_test['summary'].apply(lambda x: x.lower())\n",
    "billsum_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_semicolon(text, threshold=10):\n",
    "    '''\n",
    "    Get rid of semicolons.\n",
    "    First split text into fragments between the semicolons. If the fragment \n",
    "    is longer than the threshold, turn the semicolon into a period. O.w treat\n",
    "    it as a comma.\n",
    "    Returns new text\n",
    "    '''\n",
    "    new_text = \"\"\n",
    "    for subset in re.split(';', text):\n",
    "        subset = subset.strip() # Clear off spaces\n",
    "        # Check word count\n",
    "        if len(subset.split()) > threshold:\n",
    "            # Turn first char into uppercase\n",
    "            new_text += \". \" + subset[0].upper() + subset[1:]\n",
    "        else:\n",
    "            # Just append with a comma \n",
    "            new_text += \", \" + subset\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USC_re = re.compile('[Uu]\\.*[Ss]\\.*[Cc]\\.]+')\n",
    "PAREN_re = re.compile('\\([^(]+\\ [^\\(]+\\)')\n",
    "BAD_PUNCT_RE = re.compile(r'([%s])' % re.escape('\"#%&\\*\\+/<=>@[\\]^{|}~_'), re.UNICODE)\n",
    "BULLET_RE = re.compile('\\n[\\ \\t]*`*\\([a-zA-Z0-9]*\\)')\n",
    "DASH_RE = re.compile('--+')\n",
    "WHITESPACE_RE = re.compile('\\s+')\n",
    "EMPTY_SENT_RE = re.compile('[,\\.]\\ *[\\.,]')\n",
    "FIX_START_RE = re.compile('^[^A-Za-z]*')\n",
    "FIX_PERIOD = re.compile('\\.([A-Za-z])')\n",
    "SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\\.|\\nSEC\\.* [0-9]{1,2}\\.|Sec\\.* [0-9]{1,2}\\.')\n",
    "\n",
    "FIX_PERIOD = re.compile('\\.([A-Za-z])')\n",
    "\n",
    "SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\\.|\\nSEC\\.* [0-9]{1,2}\\.|Sec\\.* [0-9]{1,2}\\.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Borrowed from the FNDS text processing with additional logic added in.\n",
    "    Note: we do not take care of token breaking - assume SPACY's tokenizer\n",
    "    will handle this for us.\n",
    "    \"\"\"\n",
    "\n",
    "    # Indicate section headers, we need them for features\n",
    "    text = SECTION_HEADER_RE.sub('SECTION-HEADER', text)\n",
    "    # For simplicity later, remove '.' from most common acronym\n",
    "    text = text.replace(\"U.S.\", \"US\")\n",
    "    text = text.replace('SEC.', 'Section')\n",
    "    text = text.replace('Sec.', 'Section')\n",
    "    text = USC_re.sub('USC', text)\n",
    "\n",
    "    # Remove parantheticals because they are almost always references to laws \n",
    "    # We could add a special tag, but we just remove for now\n",
    "    # Note we dont get rid of nested parens because that is a complex re\n",
    "    #text = PAREN_re.sub('LAWREF', text)\n",
    "    text = PAREN_re.sub('', text)\n",
    "    \n",
    "\n",
    "    # Get rid of enums as bullets or ` as bullets\n",
    "    text = BULLET_RE.sub(' ',text)\n",
    "    \n",
    "    # Clean html \n",
    "    text = text.replace('&lt;all&gt;', '')\n",
    "\n",
    "    # Remove annoying punctuation, that's not relevant\n",
    "    text = BAD_PUNCT_RE.sub('', text)\n",
    "\n",
    "    # Get rid of long sequences of dashes - these are formating\n",
    "    text = DASH_RE.sub( ' ', text)\n",
    "\n",
    "    # removing newlines, tabs, and extra spaces.\n",
    "    text = WHITESPACE_RE.sub(' ', text)\n",
    "    \n",
    "    # If we ended up with \"empty\" sentences - get rid of them.\n",
    "    text = EMPTY_SENT_RE.sub('.', text)\n",
    "    \n",
    "    # Attempt to create sentences from bullets \n",
    "    text = replace_semicolon(text)\n",
    "    \n",
    "    # Fix weird period issues + start of text weirdness\n",
    "    #text = re.sub('\\.(?=[A-Z])', '  . ', text)\n",
    "    # Get rid of anything thats not a word from the start of the text\n",
    "    text = FIX_START_RE.sub( '', text)\n",
    "    # Sometimes periods get formatted weird, make sure there is a space between periods and start of sent   \n",
    "    text = FIX_PERIOD.sub(\". \\g<1>\", text)\n",
    "\n",
    "    # Fix quotes\n",
    "    text = text.replace('``', '\"')\n",
    "    text = text.replace('\\'\\'', '\"')\n",
    "\n",
    "    # Add special punct back in\n",
    "    text = text.replace('SECTION-HEADER', '')\n",
    "\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum_test['clean_content'] = billsum_test.text.map(clean_text)\n",
    "        \n",
    "billsum_test['clean_summary'] = billsum_test.summary.map(clean_text)\n",
    "\n",
    "billsum_test['clean_contract'] = billsum_test.contract.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = billsum_test['clean_content'].values.tolist()\n",
    "\n",
    "content_parser = [PlaintextParser.from_string(doc,Tokenizer(\"english\")) for doc in content]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting significant and redundant keywords\n",
    "Currently, this is done automatically using tf-idf however, in practice, lawyers can specify what words are significant and what are not significant. Hence, this can be customised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sent(word, sentences): \n",
    "    final = [all([w in x for w in word]) for x in sentences] \n",
    "    sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "    return int(len(sent_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(dict_elem, n):\n",
    "    result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_extraction(doc):\n",
    "    total_words = doc.split()\n",
    "    total_word_length = len(total_words)\n",
    "#     print(total_word_length)\n",
    "    \n",
    "    total_sentences = tokenize.sent_tokenize(doc)\n",
    "    total_sent_len = len(total_sentences)\n",
    "#     print(total_sent_len)\n",
    "\n",
    "    tf_score = {}\n",
    "    for each_word in total_words:\n",
    "        each_word = each_word.replace('.','')\n",
    "        if each_word not in stopWords:\n",
    "            if each_word in tf_score:\n",
    "                tf_score[each_word] += 1\n",
    "            else:\n",
    "                tf_score[each_word] = 1\n",
    "\n",
    "    # Dividing by total_word_length for each dictionary element\n",
    "    tf_score.update((x, y/int(total_word_length)) for x, y in tf_score.items())\n",
    "#     print(tf_score)\n",
    "\n",
    "    idf_score = {}\n",
    "    for each_word in total_words:\n",
    "        each_word = each_word.replace('.','')\n",
    "        if each_word not in stopWords:\n",
    "            if each_word in idf_score:\n",
    "                idf_score[each_word] = check_sent(each_word, total_sentences)\n",
    "            else:\n",
    "                idf_score[each_word] = 1\n",
    "\n",
    "    # Performing a log and divide\n",
    "    idf_score.update((x, math.log(int(total_sent_len)/y)) for x, y in idf_score.items())\n",
    "\n",
    "#     print(idf_score)\n",
    "\n",
    "    tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}\n",
    "#     print(tf_idf_score)\n",
    "\n",
    "    return get_top_n(tf_idf_score, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_keywords(top_keywords):\n",
    "    string.punctuation = \"\\!\\\"#&'()*+,./:;<=>?@[\\]^_`{|}~\" #remove $ and % as they are needed in legal documents\n",
    "    keywords = []\n",
    "    for words in top_5_keywords:\n",
    "        for word in words.keys():\n",
    "            if (word not in keywords) and ('(' not in word) and (')' not in word):\n",
    "                word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "                if word != '':\n",
    "                    keywords.append(word)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#content\n",
    "top_5_keywords = [keyword_extraction(doc.lower()) for doc in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_keywords = unique_keywords(top_5_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant = ['following',\n",
    "             'section',\n",
    "             'shall',\n",
    "             'which',\n",
    "             'the',\n",
    "             'thereof', \n",
    "             'an', \n",
    "             '-a-u-t-h-o-r-i-z-e-d', \n",
    "             'under', \n",
    "             'this', \n",
    "             '-s-u-c-h', \n",
    "             ' - ', \n",
    "             '-a-u-t-h-o-r-i-z-a-t-i-o-n', \n",
    "             'allow', \n",
    "             'held',\n",
    "            'previous',\n",
    "            'makes']\n",
    "\n",
    "significant = []\n",
    "for k in set(content_keywords):\n",
    "    if k not in redundant:\n",
    "        significant.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edmundson Algorithm Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_edmk = EdmundsonSummarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_edmk.bonus_words = tuple(significant)\n",
    "summarizer_edmk.stigma_words = tuple(redundant)\n",
    "summarizer_edmk.null_words = tuple(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "edmk_sum_train = [summarizer_edmk.key_method(doc.document, 8) for doc in content_parser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "edmk_summary = []\n",
    "\n",
    "for doc in edmk_sum_train:\n",
    "    summary = \"\"\n",
    "    for s in doc:\n",
    "        #print(type(s))\n",
    "        summary = summary + str(s) + \" \"\n",
    "    edmk_summary.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'purposes section 38, elementary secondary science, technology, engineering, mathematics (stem) contributions credit determined section taxable year equal 100 percent qualified stem contributions taxpayer taxable year. purposes section, term `qualified stem contributions\\' means \"(1) stem school contributions, \"(2) stem teacher externship expenses, \"(3) stem teacher training expenses. term `stem school contributions\\' means \"(a) stem property contributions, \"(b) stem service contributions. term `stem property contributions\\' means (but subsection (f)) allowed deduction section 170 charitable contribution stem inventory property \"(a) donee elementary secondary school described section 170(b)(1)(a)(ii), \"(b) substantially use property donee united states defense dependents\\' education educational purposes grades k-12 related purpose function donee, \"(c) original use property begins donee, \"(d) property fit productively donee\\'s education plan, \"(e) property transferred donee exchange money, property, services, shipping, installation transfer costs, \"(f) donee\\'s use disposition property accordance provisions subparagraphs (b) (e). term `stem service contributions\\' means paid incurred taxable year stem services provided united states defense dependents\\' education exclusive benefit students elementary secondary school described section 170(b)(1)(a)(ii) \"(a) taxpayer engaged trade business providing services commercial basis, \"(b) charge imposed providing services. term `stem teacher externship expenses\\' means paid incurred carry stem externship program taxpayer extent attributable participation program eligible stem teacher, including amounts paid teacher stipend participating program. term `stem externship program\\' means program \"(a) established taxpayer engaged trade business area science, technology, engineering, mathematics, \"(b) eligible stem teachers receive training enhance teaching skills areas science, technology, engineering, mathematics improve knowledge areas. term `stem teacher training expenses\\' means paid incurred taxpayer engaged trade business area science, technology, engineering, mathematics attributable participation eligible stem teacher regular training program provided employees taxpayer determined teacher\\'s school enhancing teacher\\'s teaching skills areas science, technology, engineering, mathematics. '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edmk_summary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rouge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = billsum_test['clean_summary'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "\n",
    "Scores = rouge.get_scores(edmk_summary, summary, avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1:\n",
      "precision: 0.26063258475662515\n",
      "recall: 0.43042875100100425\n",
      "f1-score: 0.3055459016032088\n",
      "\n",
      "rouge-2:\n",
      "precision: 0.125001171724707\n",
      "recall: 0.22760394531617856\n",
      "f1-score: 0.15011922095493416\n",
      "\n",
      "rouge-l:\n",
      "precision: 0.24605262716819706\n",
      "recall: 0.4044914052938941\n",
      "f1-score: 0.28795362585231266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for score, f1 in Scores.items():\n",
    "    print(f\"{score}:\")\n",
    "    print(f\"precision: {f1['p']}\")\n",
    "    print(f\"recall: {f1['r']}\")\n",
    "    print(f\"f1-score: {f1['f']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
