{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dianfarahr.2019\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import spatial\n",
    "from scipy.sparse.linalg import svds\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import rouge\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contract</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "      <td>section 1. short title. this act may be cited ...</td>\n",
       "      <td>national science education tax incentive for b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "      <td>section 1. short title. this act may be cited ...</td>\n",
       "      <td>small business expansion and hiring act of 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A bill to require the Director of National Int...</td>\n",
       "      <td>section 1. release of documents captured in ir...</td>\n",
       "      <td>requires the director of national intelligence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A bill to improve data collection and dissemin...</td>\n",
       "      <td>section 1. short title. this act may be cited ...</td>\n",
       "      <td>national cancer act of 2003 - amends the publi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A bill to amend the Internal Revenue Code of 1...</td>\n",
       "      <td>section 1. short title. this act may be cited ...</td>\n",
       "      <td>military call-up relief act - amends the inter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            contract  \\\n",
       "0  To amend the Internal Revenue Code of 1986 to ...   \n",
       "1  To amend the Internal Revenue Code of 1986 to ...   \n",
       "2  A bill to require the Director of National Int...   \n",
       "3  A bill to improve data collection and dissemin...   \n",
       "4  A bill to amend the Internal Revenue Code of 1...   \n",
       "\n",
       "                                                text  \\\n",
       "0  section 1. short title. this act may be cited ...   \n",
       "1  section 1. short title. this act may be cited ...   \n",
       "2  section 1. release of documents captured in ir...   \n",
       "3  section 1. short title. this act may be cited ...   \n",
       "4  section 1. short title. this act may be cited ...   \n",
       "\n",
       "                                             summary  \n",
       "0  national science education tax incentive for b...  \n",
       "1  small business expansion and hiring act of 201...  \n",
       "2  requires the director of national intelligence...  \n",
       "3  national cancer act of 2003 - amends the publi...  \n",
       "4  military call-up relief act - amends the inter...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billsum_test = pd.read_excel('../data/billsum_test.xlsx')\n",
    "\n",
    "billsum_test['text'] = billsum_test['text'].apply(lambda x: x.lower())\n",
    "billsum_test['summary'] = billsum_test['summary'].apply(lambda x: x.lower())\n",
    "billsum_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_semicolon(text, threshold=10):\n",
    "    '''\n",
    "    Get rid of semicolons.\n",
    "    First split text into fragments between the semicolons. If the fragment \n",
    "    is longer than the threshold, turn the semicolon into a period. O.w treat\n",
    "    it as a comma.\n",
    "    Returns new text\n",
    "    '''\n",
    "    new_text = \"\"\n",
    "    for subset in re.split(';', text):\n",
    "        subset = subset.strip() # Clear off spaces\n",
    "        # Check word count\n",
    "        if len(subset.split()) > threshold:\n",
    "            # Turn first char into uppercase\n",
    "            new_text += \". \" + subset[0].upper() + subset[1:]\n",
    "        else:\n",
    "            # Just append with a comma \n",
    "            new_text += \", \" + subset\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USC_re = re.compile('[Uu]\\.*[Ss]\\.*[Cc]\\.]+')\n",
    "PAREN_re = re.compile('\\([^(]+\\ [^\\(]+\\)')\n",
    "BAD_PUNCT_RE = re.compile(r'([%s])' % re.escape('\"#%&\\*\\+/<=>@[\\]^{|}~_'), re.UNICODE)\n",
    "BULLET_RE = re.compile('\\n[\\ \\t]*`*\\([a-zA-Z0-9]*\\)')\n",
    "DASH_RE = re.compile('--+')\n",
    "WHITESPACE_RE = re.compile('\\s+')\n",
    "EMPTY_SENT_RE = re.compile('[,\\.]\\ *[\\.,]')\n",
    "FIX_START_RE = re.compile('^[^A-Za-z]*')\n",
    "FIX_PERIOD = re.compile('\\.([A-Za-z])')\n",
    "SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\\.|\\nSEC\\.* [0-9]{1,2}\\.|Sec\\.* [0-9]{1,2}\\.')\n",
    "\n",
    "FIX_PERIOD = re.compile('\\.([A-Za-z])')\n",
    "\n",
    "SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\\.|\\nSEC\\.* [0-9]{1,2}\\.|Sec\\.* [0-9]{1,2}\\.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Borrowed from the FNDS text processing with additional logic added in.\n",
    "    Note: we do not take care of token breaking - assume SPACY's tokenizer\n",
    "    will handle this for us.\n",
    "    \"\"\"\n",
    "\n",
    "    # Indicate section headers, we need them for features\n",
    "    text = SECTION_HEADER_RE.sub('SECTION-HEADER', text)\n",
    "    # For simplicity later, remove '.' from most common acronym\n",
    "    text = text.replace(\"U.S.\", \"US\")\n",
    "    text = text.replace('SEC.', 'Section')\n",
    "    text = text.replace('Sec.', 'Section')\n",
    "    text = USC_re.sub('USC', text)\n",
    "\n",
    "    # Remove parantheticals because they are almost always references to laws \n",
    "    # We could add a special tag, but we just remove for now\n",
    "    # Note we dont get rid of nested parens because that is a complex re\n",
    "    #text = PAREN_re.sub('LAWREF', text)\n",
    "    text = PAREN_re.sub('', text)\n",
    "    \n",
    "\n",
    "    # Get rid of enums as bullets or ` as bullets\n",
    "    text = BULLET_RE.sub(' ',text)\n",
    "    \n",
    "    # Clean html \n",
    "    text = text.replace('&lt;all&gt;', '')\n",
    "\n",
    "    # Remove annoying punctuation, that's not relevant\n",
    "    text = BAD_PUNCT_RE.sub('', text)\n",
    "\n",
    "    # Get rid of long sequences of dashes - these are formating\n",
    "    text = DASH_RE.sub( ' ', text)\n",
    "\n",
    "    # removing newlines, tabs, and extra spaces.\n",
    "    text = WHITESPACE_RE.sub(' ', text)\n",
    "    \n",
    "    # If we ended up with \"empty\" sentences - get rid of them.\n",
    "    text = EMPTY_SENT_RE.sub('.', text)\n",
    "    \n",
    "    # Attempt to create sentences from bullets \n",
    "    text = replace_semicolon(text)\n",
    "    \n",
    "    # Fix weird period issues + start of text weirdness\n",
    "    #text = re.sub('\\.(?=[A-Z])', '  . ', text)\n",
    "    # Get rid of anything thats not a word from the start of the text\n",
    "    text = FIX_START_RE.sub( '', text)\n",
    "    # Sometimes periods get formatted weird, make sure there is a space between periods and start of sent   \n",
    "    text = FIX_PERIOD.sub(\". \\g<1>\", text)\n",
    "\n",
    "    # Fix quotes\n",
    "    text = text.replace('``', '\"')\n",
    "    text = text.replace('\\'\\'', '\"')\n",
    "\n",
    "    # Add special punct back in\n",
    "    text = text.replace('SECTION-HEADER', '')\n",
    "\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum_test['clean_content'] = billsum_test.text.map(clean_text)\n",
    "        \n",
    "billsum_test['clean_summary'] = billsum_test.summary.map(clean_text)\n",
    "\n",
    "billsum_test['clean_contract'] = billsum_test.contract.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = billsum_test['clean_content'].values.tolist()\n",
    "\n",
    "content_parser = [PlaintextParser.from_string(doc,Tokenizer(\"english\")) for doc in content]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LexRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_lr = LexRankSummarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sum_train = [summarizer_lr(doc.document, 8) for doc in content_parser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_summary = []\n",
    "\n",
    "for doc in lr_sum_train:\n",
    "    summary = \"\"\n",
    "    for s in doc:\n",
    "        #print(type(s))\n",
    "        summary = summary + str(s) + \" \"\n",
    "    lr_summary.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'purposes section 38, elementary secondary science, technology, engineering, mathematics (stem) contributions credit determined section taxable year equal 100 percent qualified stem contributions taxpayer taxable year. purposes section, term `qualified stem contributions\\' means \"(1) stem school contributions, \"(2) stem teacher externship expenses, \"(3) stem teacher training expenses. term `stem school contributions\\' means \"(a) stem property contributions, \"(b) stem service contributions. determination deduction section 170 purposes paragraph shall limitation section 170(e)(3)(b) applied stem inventory property. term `stem service contributions\\' means paid incurred taxable year stem services provided united states defense dependents\\' education exclusive benefit students elementary secondary school described section 170(b)(1)(a)(ii) \"(a) taxpayer engaged trade business providing services commercial basis, \"(b) charge imposed providing services. term `stem inventory property\\' means, respect contribution school, property \"(a) described paragraph (1) (2) section 1221(a) respect donor, \"(b) determined school needed school providing education grades k-12 areas science, technology, engineering, mathematics. purposes subsection, term `defense dependents\\' education system\\' means program established operated defense dependents\\' education act 1978 . amendments section shall apply taxable years beginning date enactment act. '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_summary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rouge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = billsum_test['clean_summary'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "\n",
    "Scores = rouge.get_scores(lr_summary, summary, avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1:\n",
      "precision: 0.27673588635396207\n",
      "recall: 0.4018433578734701\n",
      "f1-score: 0.30674825565409136\n",
      "\n",
      "rouge-2:\n",
      "precision: 0.12852871676430622\n",
      "recall: 0.20325683823379267\n",
      "f1-score: 0.1452357172061532\n",
      "\n",
      "rouge-l:\n",
      "precision: 0.26154769104756626\n",
      "recall: 0.37852748421655513\n",
      "f1-score: 0.28950952593283485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for score, f1 in Scores.items():\n",
    "    print(f\"{score}:\")\n",
    "    print(f\"precision: {f1['p']}\")\n",
    "    print(f\"recall: {f1['r']}\")\n",
    "    print(f\"f1-score: {f1['f']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
