{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ivyha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ivyha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import spatial\n",
    "import networkx as nx\n",
    "\n",
    "import nltk\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "from rouge_score import rouge_scorer\n",
    "from rouge import Rouge\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.kl import KLSummarizer\n",
    "from sumy.summarizers import AbstractSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('billsum_v4_1/ca_test_data_final_OFFICIAL.jsonl', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_semicolon(text, threshold=10):\n",
    "    '''\n",
    "    Get rid of semicolons.\n",
    "    First split text into fragments between the semicolons. If the fragment \n",
    "    is longer than the threshold, turn the semicolon into a period. O.w treat\n",
    "    it as a comma.\n",
    "    Returns new text\n",
    "    '''\n",
    "    new_text = \"\"\n",
    "    for subset in re.split(';', text):\n",
    "        subset = subset.strip() # Clear off spaces\n",
    "        # Check word count\n",
    "        if len(subset.split()) > threshold:\n",
    "            # Turn first char into uppercase\n",
    "            new_text += \". \" + subset[0].upper() + subset[1:]\n",
    "        else:\n",
    "            # Just append with a comma \n",
    "            new_text += \", \" + subset\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USC_re = re.compile('[Uu]\\.*[Ss]\\.*[Cc]\\.]+')\n",
    "PAREN_re = re.compile('\\([^(]+\\ [^\\(]+\\)')\n",
    "BAD_PUNCT_RE = re.compile(r'([%s])' % re.escape('\"#%&\\*\\+/<=>@[\\]^{|}~_'), re.UNICODE)\n",
    "BULLET_RE = re.compile('\\n[\\ \\t]*`*\\([a-zA-Z0-9]*\\)')\n",
    "DASH_RE = re.compile('--+')\n",
    "WHITESPACE_RE = re.compile('\\s+')\n",
    "EMPTY_SENT_RE = re.compile('[,\\.]\\ *[\\.,]')\n",
    "FIX_START_RE = re.compile('^[^A-Za-z]*')\n",
    "FIX_PERIOD = re.compile('\\.([A-Za-z])')\n",
    "SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\\.|\\nSEC\\.* [0-9]{1,2}\\.|Sec\\.* [0-9]{1,2}\\.')\n",
    "\n",
    "FIX_PERIOD = re.compile('\\.([A-Za-z])')\n",
    "\n",
    "SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\\.|\\nSEC\\.* [0-9]{1,2}\\.|Sec\\.* [0-9]{1,2}\\.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Borrowed from the FNDS text processing with additional logic added in.\n",
    "    Note: we do not take care of token breaking - assume SPACY's tokenizer\n",
    "    will handle this for us.\n",
    "    \"\"\"\n",
    "\n",
    "    # Indicate section headers, we need them for features\n",
    "    text = SECTION_HEADER_RE.sub('SECTION-HEADER', text)\n",
    "    # For simplicity later, remove '.' from most common acronym\n",
    "    text = text.replace(\"U.S.\", \"US\")\n",
    "    text = text.replace('SEC.', 'Section')\n",
    "    text = text.replace('Sec.', 'Section')\n",
    "    text = USC_re.sub('USC', text)\n",
    "\n",
    "    # Remove parantheticals because they are almost always references to laws \n",
    "    # We could add a special tag, but we just remove for now\n",
    "    # Note we dont get rid of nested parens because that is a complex re\n",
    "    #text = PAREN_re.sub('LAWREF', text)\n",
    "    text = PAREN_re.sub('', text)\n",
    "    \n",
    "\n",
    "    # Get rid of enums as bullets or ` as bullets\n",
    "    text = BULLET_RE.sub(' ',text)\n",
    "    \n",
    "    # Clean html \n",
    "    text = text.replace('&lt;all&gt;', '')\n",
    "\n",
    "    # Remove annoying punctuation, that's not relevant\n",
    "    text = BAD_PUNCT_RE.sub('', text)\n",
    "\n",
    "    # Get rid of long sequences of dashes - these are formating\n",
    "    text = DASH_RE.sub( ' ', text)\n",
    "\n",
    "    # removing newlines, tabs, and extra spaces.\n",
    "    text = WHITESPACE_RE.sub(' ', text)\n",
    "    \n",
    "    # If we ended up with \"empty\" sentences - get rid of them.\n",
    "    text = EMPTY_SENT_RE.sub('.', text)\n",
    "    \n",
    "    # Attempt to create sentences from bullets \n",
    "    text = replace_semicolon(text)\n",
    "    \n",
    "    # Fix weird period issues + start of text weirdness\n",
    "    #text = re.sub('\\.(?=[A-Z])', '  . ', text)\n",
    "    # Get rid of anything thats not a word from the start of the text\n",
    "    text = FIX_START_RE.sub( '', text)\n",
    "    # Sometimes periods get formatted weird, make sure there is a space between periods and start of sent   \n",
    "    text = FIX_PERIOD.sub(\". \\g<1>\", text)\n",
    "\n",
    "    # Fix quotes\n",
    "    text = text.replace('``', '\"')\n",
    "    text = text.replace('\\'\\'', '\"')\n",
    "\n",
    "    # Add special punct back in\n",
    "    text = text.replace('SECTION-HEADER', '')\n",
    "\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data.text.map(clean_text)\n",
    "        \n",
    "data['clean_summary'] = data.summary.map(clean_text)\n",
    "\n",
    "data['clean_title'] = data.title.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luhn Summarizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = data['clean_text'].values.tolist()\n",
    "\n",
    "content_parser = [PlaintextParser.from_string(doc,Tokenizer(\"english\")) for doc in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = LuhnSummarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "luhn_sum_train = [summarizer(doc.document, 5) for doc in content_parser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "luhn_summary = []\n",
    "\n",
    "for doc in luhn_sum_train:\n",
    "    summary = \"\"\n",
    "    for s in doc:\n",
    "        #print(type(s))\n",
    "        summary = summary + str(s) + \" \"\n",
    "    luhn_summary.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Section 501(c)(19) Internal Revenue Code related federal regulations provide exemption posts organizations war veterans, auxiliary unit society of, trust foundation for, post organization that, attributes, carries programs perpetuate memory deceased veterans members Armed Forces comfort survivors, conducts programs religious, charitable, scientific, literary, educational purposes, sponsors participates activities patriotic nature, provides social recreational activities members. Section 215.1 Revenue Taxation Code stipulates buildings, support real property buildings situated required convenient use occupation buildings, exclusively charitable purposes, owned veteransâ€™ organization chartered Congress United States, organized operated charitable purposes, solely exclusively purpose organization, conducted profit net earnings ensures benefit private individual member thereof, exempt taxation. All buildings, real property buildings situated required convenient use occupation buildings, exclusively charitable purposes, owned veteransâ€™ organization chartered Congress United States, organized operated charitable purposes, exempt federal income tax organization described Section 501(c)(19) Internal Revenue Code solely exclusively purpose organization, conducted profit net earnings inures benefit private individual member thereof, shall exempt taxation. With regard subdivision, Legislature finds declares following: The exempt activities veteransâ€™ organization described subdivision (a) qualitatively differ exempt activities nonprofit entities use property fraternal, lodge, social club purposes exempt purpose veteransâ€™ organization conduct programs perpetuate memory deceased veterans members Armed Forces comfort survivors, conduct programs religious, charitable, scientific, literary, educational purposes, sponsor participate activities patriotic nature, provide social recreational activities members. In light factors set forth subparagraphs (A) (B), use real property veteransâ€™ organization described subdivision (a) fraternal, lodge, social club purposes, constitutes exclusive use property charitable purpose meaning subdivision (b) Section 4 Article XIII California Constitution. '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luhn_summary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = data['clean_summary'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "\n",
    "Scores = rouge.get_scores(luhn_summary, summary, avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.3187697680337042,\n",
       "  'p': 0.3214156784956868,\n",
       "  'f': 0.30336462888397187},\n",
       " 'rouge-2': {'r': 0.15411982992108747,\n",
       "  'p': 0.16058954258756727,\n",
       "  'f': 0.1471050918152946},\n",
       " 'rouge-l': {'r': 0.29897174674694754,\n",
       "  'p': 0.3027902355722908,\n",
       "  'f': 0.28515365653969627}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scores"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "097a94cc1bfc5a0a380b63d1eb45dc603bb0f63840061a2276c4020d01d7c102"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
